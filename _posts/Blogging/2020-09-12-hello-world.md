---
title: "Test"
categories: 
  - Blogging
last_modified_at: 2020-09-12
tags:
  - Test
use_math: true
toc: true
---
#### Transformer-layer distillation

* Loss for attention mechanism


\[ y=ax+b \]


수식은 $ y=cx+d $ 이렇게 됩니다.

$$\alpha = \beta$$  
  
$$y=x^2$$

\[ e^{i\pi} + 1 = 0 \]

\[ e^x=\sum_{i=0}^\infty \frac{1}{i!}x^i \]

\[ \frac{n!}{k!(n-k)!} = {n \choose k} \]



* 아아아아ㅏ 테스트 중입니다 
