---
title: "Test"
categories: 
  - Blogging
last_modified_at: 2020-09-10
tags:
  - Test
use_math: true
toc: true
---
#### Transformer-layer distillation

* Loss for attention mechanism

$$
K(a,b) = \int \mathcal{D}x(t) \exp(2\pi i S[x]/\hbar)
$$

* 특이한 점은 softmax를 타기 전의 matrix를 loss function의 input으로 넣는 것인데, 이게 수렴이 더 빠르게 되었다고 한다.
